“””
vt_live_cei_experiment.py — Live CEI Trace on Llama-3.1-8B

Accompanies: “Adaptive Reasoning Orchestration: A Variability Theory Engine
for Real-Time Stagnation Detection, Adaptive Escalation, and Field Transformation”
Author: Julia Ryzhkova (2026)

Extracts hidden states from Llama-3.1-8B during generation, computes the
Cognitive Entropy Index (Equation 4) in real time, and demonstrates
escalation triggers on problems where the model stagnates.

Requirements:
pip install torch transformers accelerate matplotlib numpy

Hardware: GPU with >=16GB VRAM (A100, L4, RTX 4090)

- For Colab Pro: runtime → change runtime type → A100
- For RunPod/Lambda: any A100 or L40S instance

Usage:
python vt_live_cei_experiment.py

```
# Or with custom model/problem:
python vt_live_cei_experiment.py --model meta-llama/Llama-3.1-8B-Instruct \\
                                 --problem tower_of_hanoi \\
                                 --max_tokens 1024
```

Outputs:

- vt_live_cei_trace.png (Figure 3 candidate)
- Console: full trace with Iₜ values and escalation markers
- vt_live_cei_data.json (raw data for reproducibility)
“””

import argparse
import json
import time
import numpy as np
import torch
import matplotlib.pyplot as plt
from collections import deque

# ============================================================

# VT Engine Parameters (same as synthetic simulations)

# ============================================================

T = 10              # sliding window size
K = 5               # persistence parameter
THRESHOLDS = [0.55, 0.70, 0.82, 0.92]  # θ₀–θ₃
LAYER_FRACTION = 0.75  # extract from this fraction of depth (0.75 = layer 24/32)

# ============================================================

# Test Problems — designed to induce stagnation

# ============================================================

PROBLEMS = {
“tower_of_hanoi”: {
“system”: “You are a precise logical reasoner. Show every move explicitly.”,
“prompt”: (
“Solve the Tower of Hanoi with 6 disks. Pegs are A, B, C. “
“All disks start on peg A. Move all to peg C. “
“Rules: only move one disk at a time, never place larger on smaller. “
“Show every single move as ‘Move disk N from X to Y’. “
“Think step by step.”
),
“description”: “Tower of Hanoi (6 disks = 63 moves). Models typically stagnate around move 15-25.”,
},
“gsm8k_hard”: {
“system”: “You are a math tutor. Show all work step by step.”,
“prompt”: (
“A store sells apples for $1.50 each. They offer a deal: buy 3, get the 4th at 50% off. “
“Sarah wants to buy apples for a party. She needs at least 47 apples. She has $65. “
“Tax is 8.5% on the total after discounts. “
“How many apples can she actually buy? “
“Will she have enough? If not, how much more does she need? “
“Show every calculation step.”
),
“description”: “Multi-step arithmetic with discount, tax, and constraint checking.”,
},
“blocks_world”: {
“system”: “You are a planning agent. Think through each step carefully.”,
“prompt”: (
“Blocks World problem. You have blocks A, B, C, D, E, F, G on a table.\n”
“Initial state: A is on table. B is on A. C is on B. D is on table. E is on D. “
“F is on table. G is on F.\n”
“Goal state: A on B, B on C, C on D, D on E, E on F, F on G, G on table.\n”
“Rules: can only move top block, can only place on table or on top of a stack.\n”
“Find the optimal sequence of moves. Show each move and the state after it.”
),
“description”: “7-block planning problem. Requires backtracking, models often loop.”,
},
“simple_baseline”: {
“system”: “You are a helpful assistant.”,
“prompt”: “What are three interesting facts about dolphins?”,
“description”: “Easy baseline — model should NOT stagnate. Iₜ should stay low.”,
},
}

# ============================================================

# Hidden State Extraction via PyTorch Hooks

# ============================================================

class HiddenStateCollector:
“”“Attaches a forward hook to a specific transformer layer
and collects the hidden state for the last generated token.”””

```
def __init__(self, model, layer_index=None):
    self.model = model
    self.hidden_states = []
    self._hook_handle = None

    # Determine layer to hook
    if layer_index is None:
        n_layers = model.config.num_hidden_layers
        layer_index = int(n_layers * LAYER_FRACTION)
    self.layer_index = layer_index

    # Attach hook
    layer = model.model.layers[layer_index]
    self._hook_handle = layer.register_forward_hook(self._hook_fn)
    print(f"Hook attached to layer {layer_index}/{model.config.num_hidden_layers}")

def _hook_fn(self, module, input, output):
    """Extract hidden state of the LAST token (the one being generated)."""
    # output is a tuple; first element is hidden_states tensor
    # Shape: (batch_size, seq_len, hidden_dim)
    if isinstance(output, tuple):
        hidden = output[0]
    else:
        hidden = output

    # Take the last token's hidden state
    last_token_hidden = hidden[:, -1, :].detach().cpu().float().numpy()
    self.hidden_states.append(last_token_hidden.squeeze())

def remove_hook(self):
    if self._hook_handle:
        self._hook_handle.remove()

def get_states(self):
    return np.array(self.hidden_states)
```

# ============================================================

# Real-time CEI Computation (Equation 4)

# ============================================================

class LiveCEI:
“”“Computes Cognitive Entropy Index in real time as tokens are generated.”””

```
def __init__(self, window_size=T):
    self.window_size = window_size
    self.buffer = deque(maxlen=window_size)
    self.all_distances = []  # for global d_max tracking
    self.inertia_history = []
    self.d_max = 1e-10  # running d_max (updated as we go)

def _cosine_distance(self, a, b):
    na, nb = np.linalg.norm(a), np.linalg.norm(b)
    if na == 0 or nb == 0:
        return 1.0
    return 1.0 - np.clip(np.dot(a, b) / (na * nb), -1.0, 1.0)

def update(self, hidden_state):
    """Add new hidden state and compute current Iₜ."""
    self.buffer.append(hidden_state)

    if len(self.buffer) < 2:
        self.inertia_history.append(0.0)
        return 0.0

    window = np.array(list(self.buffer))
    centroid = np.mean(window, axis=0)

    distances = []
    for v in window:
        d = self._cosine_distance(v, centroid)
        distances.append(d)
        # Update running d_max
        if d > self.d_max:
            self.d_max = d

    mean_dist = np.mean(distances)
    I_t = 1.0 - (mean_dist / self.d_max) if self.d_max > 1e-10 else 0.0
    I_t = np.clip(I_t, 0.0, 1.0)

    self.inertia_history.append(I_t)
    return I_t
```

# ============================================================

# Escalation Engine (Equations 5-6)

# ============================================================

class EscalationEngine:
“”“Tracks escalation state based on CEI values.”””

```
def __init__(self, thresholds=THRESHOLDS, k=K):
    self.thresholds = thresholds
    self.k = k
    self.current_level = 0
    self.level_history = []
    self.escalation_events = []  # (token_index, new_level, I_t)
    self.consecutive_above = 0

def update(self, token_index, I_t):
    """Check escalation condition and update level."""
    if self.current_level < len(self.thresholds):
        threshold = self.thresholds[self.current_level]
        if I_t > threshold:
            self.consecutive_above += 1
        else:
            self.consecutive_above = 0

        if self.consecutive_above >= self.k:
            self.current_level += 1
            self.escalation_events.append({
                'token': token_index,
                'level': self.current_level,
                'I_t': float(I_t),
                'threshold_crossed': float(threshold),
            })
            self.consecutive_above = 0
            print(f"  ⚡ ESCALATION → L{self.current_level} at token {token_index} "
                  f"(Iₜ={I_t:.3f}, θ={threshold})")

    self.level_history.append(self.current_level)
    return self.current_level
```

# ============================================================

# Token-by-Token Generation with CEI Monitoring

# ============================================================

def generate_with_cei(model, tokenizer, messages, max_new_tokens=512):
“”“Generate tokens one by one, computing CEI at each step.”””

```
# Prepare input
input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
input_ids = inputs["input_ids"]
prompt_length = input_ids.shape[1]

# Initialize components
collector = HiddenStateCollector(model)
cei = LiveCEI()
escalation = EscalationEngine()

# Storage
generated_tokens = []
generated_text_parts = []
token_data = []

print(f"\\nGenerating (prompt length: {prompt_length} tokens)...")
print("-" * 60)

start_time = time.time()

with torch.no_grad():
    for step in range(max_new_tokens):
        # Forward pass
        outputs = model(input_ids)
        next_token_logits = outputs.logits[:, -1, :]

        # Sample (greedy for reproducibility)
        next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)

        # Check for EOS
        if next_token_id.item() == tokenizer.eos_token_id:
            print(f"\\n[EOS at token {step}]")
            break

        # Decode token
        token_text = tokenizer.decode(next_token_id[0], skip_special_tokens=True)
        generated_tokens.append(next_token_id.item())
        generated_text_parts.append(token_text)

        # Get hidden state (hook already captured it)
        hidden = collector.hidden_states[-1] if collector.hidden_states else None

        if hidden is not None:
            # Compute CEI
            I_t = cei.update(hidden)
            # Check escalation
            level = escalation.update(step, I_t)

            # Store data point
            token_data.append({
                'step': step,
                'token_id': next_token_id.item(),
                'token_text': token_text,
                'I_t': float(I_t),
                'level': level,
            })

            # Print progress every 20 tokens
            if step % 20 == 0:
                elapsed = time.time() - start_time
                text_so_far = "".join(generated_text_parts[-30:])
                print(f"  [{step:4d}] Iₜ={I_t:.3f} L{level} "
                      f"({elapsed:.1f}s) ...{text_so_far[-50:]}")

        # Append token and continue
        input_ids = torch.cat([input_ids, next_token_id], dim=-1)

collector.remove_hook()
elapsed = time.time() - start_time
full_text = "".join(generated_text_parts)

print("-" * 60)
print(f"Generated {len(generated_tokens)} tokens in {elapsed:.1f}s")
print(f"Final level: L{escalation.current_level}")
print(f"Escalation events: {len(escalation.escalation_events)}")

return {
    'token_data': token_data,
    'full_text': full_text,
    'inertia_history': cei.inertia_history,
    'level_history': escalation.level_history,
    'escalation_events': escalation.escalation_events,
    'n_tokens': len(generated_tokens),
    'time_seconds': elapsed,
}
```

# ============================================================

# Figure 3: Live CEI Trace Visualization

# ============================================================

def plot_trace(result, problem_name, save_path=‘vt_live_cei_trace.png’):
“”“Generate Figure 3 for the paper.”””

```
I_hist = result['inertia_history']
L_hist = result['level_history']
esc_events = result['escalation_events']
n = len(I_hist)

if n == 0:
    print("No data to plot.")
    return

fig, axes = plt.subplots(3, 1, figsize=(14, 10), gridspec_kw={'height_ratios': [3, 1.5, 1.5]})
fig.suptitle(f'Figure 3. Live CEI Trace — Llama-3.1-8B on {problem_name}',
             fontsize=14, fontweight='bold', y=0.98)

steps = range(n)

# Panel 1: Iₜ time series with thresholds
ax = axes[0]
ax.plot(steps, I_hist, color='#1976D2', linewidth=1.2, label='$I_t$ (live)')
for i, th in enumerate(THRESHOLDS):
    ax.axhline(y=th, color='gray', linestyle='--', alpha=0.5, linewidth=0.8)
    ax.text(n + 1, th, f'$\\\\theta_{i}$={th}', fontsize=8, va='center', color='gray')
for ev in esc_events:
    ax.axvline(x=ev['token'], color='#FF5722', alpha=0.7, linewidth=1.5)
    ax.annotate(f"→L{ev['level']}", xy=(ev['token'], ev['I_t']),
               fontsize=8, color='#FF5722', fontweight='bold',
               xytext=(5, 10), textcoords='offset points')
ax.set_ylabel('Cognitive Entropy Index ($I_t$)', fontsize=11)
ax.set_ylim(-0.05, 1.1)
ax.set_xlim(0, n)
ax.legend(fontsize=9, loc='upper left')
ax.grid(True, alpha=0.3)

# Panel 2: Escalation level
ax = axes[1]
ax.step(steps[:len(L_hist)], L_hist, color='#D32F2F', linewidth=2, where='post')
for ev in esc_events:
    ax.plot(ev['token'], ev['level'], 'o', color='#FF5722', markersize=10, zorder=5)
ax.set_ylabel('Escalation Level', fontsize=11)
ax.set_ylim(-0.3, 4.5)
ax.set_yticks([0, 1, 2, 3, 4])
ax.set_yticklabels(['L0', 'L1', 'L2', 'L3', 'L4'])
ax.set_xlim(0, n)
ax.grid(True, alpha=0.3)

# Panel 3: Text annotations at key moments
ax = axes[2]
ax.set_xlim(0, n)
ax.set_ylim(0, 1)
ax.axis('off')
ax.set_title('Generated text at escalation points', fontsize=10, fontstyle='italic', loc='left')

token_data = result['token_data']
if esc_events and token_data:
    for i, ev in enumerate(esc_events[:4]):  # max 4 annotations
        tok_idx = ev['token']
        # Get surrounding text
        start = max(0, tok_idx - 15)
        end = min(len(token_data), tok_idx + 15)
        context_tokens = [td['token_text'] for td in token_data[start:end]]
        context = "".join(context_tokens)
        if len(context) > 80:
            context = "..." + context[-77:]

        y_pos = 0.85 - i * 0.22
        ax.text(0.02, y_pos, f"L{ev['level']} (token {tok_idx}):",
               fontsize=9, fontweight='bold', color='#D32F2F',
               transform=ax.transAxes)
        ax.text(0.15, y_pos, f'"{context}"',
               fontsize=8, fontstyle='italic', color='#333',
               transform=ax.transAxes, wrap=True)
else:
    ax.text(0.5, 0.5, 'No escalation triggered (healthy exploration)',
           fontsize=11, ha='center', va='center', color='#388E3C',
           transform=ax.transAxes)

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.savefig(save_path, dpi=300, bbox_inches='tight')
plt.close()
print(f"\\nFigure saved: {save_path}")
```

# ============================================================

# Main

# ============================================================

def main():
parser = argparse.ArgumentParser(description=‘VT Engine Live CEI Experiment’)
parser.add_argument(’–model’, type=str, default=‘meta-llama/Llama-3.1-8B-Instruct’,
help=‘HuggingFace model ID’)
parser.add_argument(’–problem’, type=str, default=‘tower_of_hanoi’,
choices=list(PROBLEMS.keys()),
help=‘Test problem to use’)
parser.add_argument(’–max_tokens’, type=int, default=512,
help=‘Maximum tokens to generate’)
parser.add_argument(’–run_all’, action=‘store_true’,
help=‘Run all problems (for comprehensive Figure 3)’)
parser.add_argument(’–output_dir’, type=str, default=’.’,
help=‘Directory for output files’)
args = parser.parse_args()

```
# Check GPU
if not torch.cuda.is_available():
    print("WARNING: No GPU detected. This will be very slow on CPU.")
    print("Recommended: run on Google Colab Pro (A100) or similar.")
    device = "cpu"
else:
    device = "cuda"
    gpu_name = torch.cuda.get_device_name(0)
    gpu_mem = torch.cuda.get_device_properties(0).total_mem / 1e9
    print(f"GPU: {gpu_name} ({gpu_mem:.1f} GB)")

# Load model
print(f"\\nLoading {args.model}...")
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(args.model)
model = AutoModelForCausalLM.from_pretrained(
    args.model,
    torch_dtype=torch.float16,
    device_map="auto",
    attn_implementation="eager",  # needed for hook access
)
model.eval()
print(f"Model loaded. Hidden dim: {model.config.hidden_size}, "
      f"Layers: {model.config.num_hidden_layers}")

# Run experiments
problems_to_run = list(PROBLEMS.keys()) if args.run_all else [args.problem]
all_results = {}

for problem_name in problems_to_run:
    problem = PROBLEMS[problem_name]
    print(f"\\n{'='*60}")
    print(f"Problem: {problem_name}")
    print(f"Description: {problem['description']}")
    print(f"{'='*60}")

    messages = [
        {"role": "system", "content": problem["system"]},
        {"role": "user", "content": problem["prompt"]},
    ]

    result = generate_with_cei(model, tokenizer, messages, args.max_tokens)
    result['problem_name'] = problem_name
    result['problem_description'] = problem['description']
    result['model'] = args.model
    all_results[problem_name] = result

    # Plot individual trace
    plot_path = f"{args.output_dir}/vt_live_cei_{problem_name}.png"
    plot_trace(result, problem_name, save_path=plot_path)

# Save raw data
data_path = f"{args.output_dir}/vt_live_cei_data.json"
# Convert numpy types for JSON serialization
serializable = {}
for k, v in all_results.items():
    sv = {**v}
    sv['inertia_history'] = [float(x) for x in v['inertia_history']]
    sv['level_history'] = [int(x) for x in v['level_history']]
    # Remove full text from JSON (too long), keep first/last 200 chars
    if len(sv['full_text']) > 400:
        sv['full_text_preview'] = sv['full_text'][:200] + " ... " + sv['full_text'][-200:]
    del sv['full_text']
    serializable[k] = sv

with open(data_path, 'w') as f:
    json.dump(serializable, f, indent=2, default=str)
print(f"\\nRaw data saved: {data_path}")

# Summary
print(f"\\n{'='*60}")
print("EXPERIMENT SUMMARY")
print(f"{'='*60}")
for name, r in all_results.items():
    print(f"\\n{name}:")
    print(f"  Tokens generated: {r['n_tokens']}")
    print(f"  Final level: L{r['level_history'][-1] if r['level_history'] else 0}")
    print(f"  Escalation events: {len(r['escalation_events'])}")
    if r['inertia_history']:
        print(f"  Mean Iₜ: {np.mean(r['inertia_history']):.3f}")
        print(f"  Max Iₜ: {np.max(r['inertia_history']):.3f}")
    for ev in r['escalation_events']:
        print(f"  ⚡ L{ev['level']} at token {ev['token']} (Iₜ={ev['I_t']:.3f})")
```

if **name** == ‘**main**’:
main()
